<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Fisher information and CRLB | Don&#39;t Panic!</title>
  <meta name="author" content="Farseer He">
  
  <meta name="description" content="Fisher Information又来抄录 Wikipedia 了，Fisher information 这个词我在好多地方见过，一直是不求甚解地意会着，最近发现这影响到一些东西的理解，于是开始翻看百科。
言归正传，Fisher information 对于学统计的人应该很熟悉，用于描述可观测的变量 X 包含的对于决定它分布的参数 $\theta$ 的信息大小，直观地理解，如果 X 包含的关于 $\theta$ 的信息越多，后者就越确定，也就是其估计的方差越小。了解到这个信息就能得知，对于某些特定的分布模型，我们对于参数 $\theta$ 的极大似然估计 $\hat{\theta}_{ML}$ 的确信程度。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Fisher information and CRLB"/>
  <meta property="og:site_name" content="Don&#39;t Panic!"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Don&#39;t Panic!" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-40313156-5', 'auto');
	ga('send', 'pageview');

</script>


</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Don&#39;t Panic!</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-10-09T16:00:00.000Z"><a href="/2015/10/10/fisher-information-and-crlb/">2015-10-10</a></time>
      
      
  
    <h1 class="title">Fisher information and CRLB</h1>
  

    </header>
    <div class="entry">
      
        <h2 id="Fisher-Information"><a href="#Fisher-Information" class="headerlink" title="Fisher Information"></a>Fisher Information</h2><p>又来抄录 Wikipedia 了，Fisher information 这个词我在好多地方见过，一直是不求甚解地意会着，最近发现这影响到一些东西的理解，于是开始翻看百科。</p>
<p>言归正传，Fisher information 对于学统计的人应该很熟悉，用于描述可观测的变量 X 包含的对于决定它分布的参数 $\theta$ 的信息大小，直观地理解，如果 X 包含的关于 $\theta$ 的信息越多，后者就越确定，也就是其估计的方差越小。了解到这个信息就能得知，对于某些特定的分布模型，我们对于参数 $\theta$ 的极大似然估计 $\hat{\theta}_{ML}$ 的确信程度。</p>
<a id="more"></a>
<p>更加精确地：</p>
<ul>
<li>X 是一个可观测的随机变量</li>
<li>X 的分布由未知参数 $\theta$ 决定</li>
<li>似然函数（likelihood function） 表示为 $f(x; \theta) = P(x | \theta)$</li>
</ul>
<p>Fisher information 的定义如下：</p>
<p>$$<br>\mathcal{I}(\theta)=\operatorname{E} \left[\left. \left(\frac{\partial}{\partial\theta} \log f(X;\theta)\right) ^2\right|\theta \right] = \int \left(\frac{\partial}{\partial\theta} \log f(x;\theta)\right) ^2 f(x; \theta)\; \mathrm{d}x \\<br>= - \operatorname{E} \left[\left. \frac{\partial ^2}{\partial\theta ^2} \log f(X;\theta)\right|\theta \right] \tag{1}\label{eq1}<br>$$</p>
<p>后一个等号成立是因为：</p>
<p>$$<br>\frac{\partial ^2}{\partial\theta ^2} \log f(X;\theta)=<br>\frac{\frac{\partial ^2}{\partial\theta ^2} f(X;\theta)}{f(X; \theta)}<br>\;-\;<br>\left( \frac{\frac{\partial}{\partial\theta} f(X;\theta)}{f(X; \theta)} \right) ^2=<br>\frac{\frac{\partial ^2}{\partial\theta ^2} f(X;\theta)}{f(X; \theta)}<br>\;-\;<br>\left( \frac{\partial}{\partial\theta} \log f(X;\theta)\right) ^2<br>\\<br>\operatorname{E} \left[\left. \frac{\frac{\partial ^2}{\partial\theta ^2} f(X;\theta)}{f(X; \theta)}\right|\theta \right]=\int \frac{\frac{\partial ^2}{\partial\theta ^2} f(x;\theta)}{f(x;\theta)}f(x;\theta)\; \mathrm{d}x=<br>\frac{\partial ^2}{\partial\theta ^2} \int f(x; \theta)\; \mathrm{d}x=<br>\frac{\partial ^2}{\partial\theta ^2} \; 1 = 0.<br>$$</p>
<p>注意：</p>
<ul>
<li>X 表示随机变量，而 x 表示该变量的特定取值</li>
<li>取期望值的时候 $\theta$ 保持不变，表示其真实值</li>
<li>似然函数表示对应的条件概率密度，故 $\int f(x;\theta)\;\mathrm{d}x = 1$</li>
</ul>
<h3 id="The-Intuition"><a href="#The-Intuition" class="headerlink" title="The Intuition"></a>The Intuition</h3><p>首先来看为什么上边定义的 information 能够表示上边提到的 “X 包含的对于决定它分布的参数 $\theta$ 的信息大小”。</p>
<p>我们知道后验概率 $P(\theta|x) \propto P(x|\theta) P(\theta)$，当先验分布为均匀分布时（即没有任何对参数的先验知识），$P(\theta|x) \propto f(x;\theta)$</p>
<p><img src="/assets/images/article/fisher.gif" alt="Fisher"></p>
<p>上图中，对于两组特定的样本，其后验概率密度函数的形状与图中的曲线相同，较为陡峭的蓝线在其极大似然的参数 $\hat{\theta}_{ML}$ 附近的变化剧烈，对应了参数较为确定，也就是“X 包含了更多关于参数的信息”。而 Fisher information 恰好能够描述这种陡峭程度，由于：</p>
<ol>
<li>考虑足够多样本的情形，由于 MLE 是 <a href="https://en.wikipedia.org/wiki/Consistent_estimator" target="_blank" rel="external">consistent estimator</a>, $\hat{\theta}_{ML} \to \theta$</li>
<li>假设 log likelihood 二阶导数的期望近似于 log likelihood 的期望的二阶导数，而后者为一个以 $\theta$ 为极值的函数的二阶导数（在 $\theta$ 处）。</li>
<li>Fisher information 近似描述了后验概率的 log 期望在 MLE 处的二阶导数的大小，即陡峭程度。</li>
</ol>
<h2 id="Properties-of-MLE"><a href="#Properties-of-MLE" class="headerlink" title="Properties of MLE"></a>Properties of MLE</h2><p>顺便提一下有关 MLE (Maximum Likelihood Estimator) 的两个重要属性，这也与 Fisher information 密切相关。</p>
<h3 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h3><p>说的是，当样本足够多时，MLE 趋近于真实参数值，简单证明如下：</p>
<ol>
<li>与前文标记略有不同，此处令真实的参数值为 $\theta_0$, 且定义<br>$$<br>L(\theta) = E_{\theta_0} l(X|\theta) = \operatorname{E} \left[\left. log\ f(X;\theta)\right|\theta_0 \right] = \int (log f(x; \theta)) f(x; \theta_0) \mathrm{d}x<br>$$</li>
<li>根据 MLE 的定义<br>$$<br>\hat{\theta}_{ML} = \underset{\theta}{\operatorname{argmax}} L_n(\theta) = \underset{\theta}{\operatorname{argmax}} \frac1n \sum\limits_{i=1} ^n log\ f(x_i; \theta)<br>$$</li>
<li>根据<a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" target="_blank" rel="external">大数定理</a>，有 $L_n(\theta) \to L(\theta)$</li>
<li>根据<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="external">K-L divergence</a>，有 $\underset{\theta}{\operatorname{argmax}} L(\theta) = \theta_0$</li>
<li>根据 2，3，4 可得 $\hat{\theta}_{ML} \to \theta_0$</li>
</ol>
<h3 id="Asymptotic-normality"><a href="#Asymptotic-normality" class="headerlink" title="Asymptotic normality"></a>Asymptotic normality</h3><p>$\sqrt{n}(\hat{\theta}_{ML} - \theta_0) \to ^d N(0, 1/\mathcal{I}(\theta_0))$，在 consistency 的基础上，表明了 MLE 服从高斯分布，其方差为 Fisher information 的倒数。简单证明如下：</p>
<ol>
<li>根据上边的1得到 $L’_n(\hat{\theta}_{ML}) = 0$，同理有 $L’(\theta_0) = E_{\theta_0} l’(X|\theta_0) = 0$</li>
<li>不妨设 $\theta_0 \leq \hat{\theta}_{ML}$, 根据<a href="https://en.wikipedia.org/wiki/Mean_value_theorem" target="_blank" rel="external">微分中值定理</a><br>$$<br>\exists \theta_1 \in [\theta, \theta_0], 0 = L’_n(\hat{\theta}_{ML}) = L’_n(\theta_0) + L’’(\theta_1)(\hat{\theta}_{ML} - \theta_0)<br>$$</li>
<li>于是<br>$$<br>\sqrt{n}(\hat{\theta}_{ML} - \theta_0) = - \frac{\sqrt{n}L’_n(\theta_0)}{L’’_n(\theta_1)}<br>$$</li>
<li>根据<a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="external">中心极限定理</a><br>$$<br>\sqrt{n}L’_n(\theta_0) = \sqrt{n}(\frac1n \sum\limits_{i=1} ^ n f’(x_i; \theta_0) - 0) = \sqrt{n}(\frac1n \sum\limits_{i=1} ^ n f’(x_i; \theta_0) - E_{\theta_0}l’(X|\theta_0)) \to N(0, var_{\theta_0}(l’(X|\theta_0)))<br>$$</li>
<li>根据大数定理（LLN）<br>$$<br>L’’_n(\theta) = \frac1n \sum l’’(X_i|\theta) \to E_{\theta_0}l’’(X|\theta)<br>$$<br>又 $\hat{\theta}_{ML} \to \theta_0, \theta_1 \to \theta_0$，于是<br>$$<br>l’’_n(\theta_1) \to E_{\theta_0}l’’(X|\theta_0) = -\mathcal{I}(\theta_0)<br>$$</li>
<li>根据 4，5 可得<br>$$-\frac{\sqrt{n}L’_n(\theta_0)}{L’’_n(\theta_1)} \to ^d N(0, \frac{var_{\theta_0}(l’(X|\theta_0))}{\mathcal{I}(\theta_0) ^2})<br>$$</li>
<li>又根据 1 以及 $\eqref{eq1}$<br>$$<br>var_{\theta_0}(l’(X|\theta_0)) = E_{\theta_0}(l’(X|\theta_0)) ^2 - (E_{\theta_0}l’(X|\theta_0)) ^2 = \mathcal{I}(\theta_0) - 0<br>$$<br>再根据 3，6 即得结论</li>
</ol>
<h3 id="Overview-of-the-Proofs"><a href="#Overview-of-the-Proofs" class="headerlink" title="Overview of the Proofs"></a>Overview of the Proofs</h3><p>用一张图表示各个重要结论之间的导出关系：</p>
<p><img src="/assets/images/article/theorems-ov.png" alt="ov"></p>
<p>至于具体的证明细节，就不长篇抄袭了……</p>
<h2 id="Cramer–Rao-Lower-Bound"><a href="#Cramer–Rao-Lower-Bound" class="headerlink" title="Cramér–Rao Lower Bound"></a>Cramér–Rao Lower Bound</h2><p>另一个关于 Fisher information 的重要结论就是 Cramér–Rao inequality:<br>对于任意的关于 $\theta$ 的无偏估计 $\hat{\theta}$，必然有 $var(\hat{\theta}) \geq 1/\mathcal{I(\theta)}$.</p>
<p>更一般地，若有偏估计 T 满足 $E(T) = \psi(\theta)$，则 $var(T) \geq \frac{\psi’(\theta) ^2} {\mathcal{I(\theta)}}$.</p>
<h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>对上面的一般结论进行简短的证明：</p>
<ol>
<li>令<br>$$<br>V = l’(\theta) = \frac{\partial}{\partial\theta} log f(X;\theta) = \frac{f’(X;\theta)}{f(X;\theta)}<br>$$<br>有<br>$$<br>E(V) = \int_x f(x;\theta)[\frac{1}{f(x;\theta)}\frac{\partial}{\partial\theta}f(x;\theta)] \mathrm{d}x = \frac{\partial}{\partial\theta} \int_x f(x;\theta)\mathrm{d}x = 0<br>$$</li>
<li>根据 1<br>$$cov(V, T) = E(VT - E(V)T - E(T)V + E(V)E(T)) = E(VT) - E(T)E(V) = E(VT)<br>$$</li>
<li>$$<br>cov(V, T) = E(VT) = \int_x t(x)[\frac{\partial}{\partial\theta}f(x;\theta)] \mathrm{d}x = \frac{\partial}{\partial\theta}[\int_x t(x)f(x;\theta)\mathrm{d}x] = \psi’(\theta)<br>$$</li>
<li>根据<a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality" target="_blank" rel="external">柯西-洗袜子不等式</a><br>$$<br>var(T)var(V) \geq cov(V, T) ^2 = \psi’(\theta) ^2<br>$$</li>
<li>根据定义 $\eqref{eq1}$，$var(V) = \mathcal{I}(\theta)$，带入 4 即得结论</li>
</ol>
<h3 id="Multivariate-Extention"><a href="#Multivariate-Extention" class="headerlink" title="Multivariate Extention"></a>Multivariate Extention</h3><p>论文中出现较多的情况是多参数的 CRLB, 定义如下：</p>
<ol>
<li>参数： $\boldsymbol{\theta} = \left[ \theta_1, \theta_2, \dots, \theta_d \right] ^T \in \mathbb{R} ^d$</li>
<li>Fisher information matrix 是一个 d x d 的矩阵，元素<br>$$<br>I_{m, k}<br>= \mathrm{E} \left[<br>\frac{\partial }{\partial \theta_m} \log f\left(x; \boldsymbol{\theta}\right)<br>\frac{\partial }{\partial \theta_k} \log f\left(x; \boldsymbol{\theta}\right)<br>\right] = -\mathrm{E} \left[<br>\frac{\partial  ^2}{\partial \theta_m \partial \theta_k} \log f\left(x; \boldsymbol{\theta}\right)<br>\right].<br>$$</li>
<li>偏估计 $\boldsymbol{T}(X) = (T_1(X), \ldots, T_d(X)) ^T$ 满足 $\mathrm{E}(\boldsymbol{T}(X)) = \boldsymbol{\psi}(\boldsymbol{\theta})$</li>
</ol>
<p>有结论如下：</p>
<p>$$<br>\mathrm{cov}_{\boldsymbol{\theta}}\left(\boldsymbol{T}(X)\right) \geq<br>\frac {\partial \boldsymbol{\psi} \left(\boldsymbol{\theta}\right)}<br>{\partial \boldsymbol{\theta}} [I\left(\boldsymbol{\theta}\right)] ^{-1}<br>\left( \frac {\partial \boldsymbol{\psi}\left(\boldsymbol{\theta}\right)}<br>{\partial \boldsymbol{\theta}}<br>\right) ^T<br>$$</p>
<p>证明略过，抄数学公式也是累…</p>
<p>综合 CRLB 以及 MLE 的 Asymptotic Normality 性质，可知当样本足够时，MLE 趋向于最优估计（无偏，且方差最小）。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Fisher_information" target="_blank" rel="external">Fisher information</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound#General_scalar_case" target="_blank" rel="external">Cramér–Rao bound</a></li>
<li><a href="http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf" target="_blank" rel="external">http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf</a></li>
</ul>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/ML/">ML</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/math/">math</a>, <a href="/tags/statistics/">statistics</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
    
      <a class="addthis_button_tweet"></a>
    
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:farseer.cn">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/ML/">ML</a><small>6</small></li>
  
    <li><a href="/categories/PLT/">PLT</a><small>4</small></li>
  
    <li><a href="/categories/TCS/">TCS</a><small>2</small></li>
  
    <li><a href="/categories/fun/">fun</a><small>2</small></li>
  
    <li><a href="/categories/notes/">notes</a><small>1</small></li>
  
    <li><a href="/categories/tips/">tips</a><small>2</small></li>
  
    <li><a href="/categories/tweak/">tweak</a><small>3</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Coq/">Coq</a><small>1</small></li>
  
    <li><a href="/tags/EVA/">EVA</a><small>1</small></li>
  
    <li><a href="/tags/Information-theory/">Information theory</a><small>2</small></li>
  
    <li><a href="/tags/Physics/">Physics</a><small>0</small></li>
  
    <li><a href="/tags/art/">art</a><small>1</small></li>
  
    <li><a href="/tags/distributed/">distributed</a><small>2</small></li>
  
    <li><a href="/tags/game/">game</a><small>1</small></li>
  
    <li><a href="/tags/haskell/">haskell</a><small>2</small></li>
  
    <li><a href="/tags/javascript/">javascript</a><small>2</small></li>
  
    <li><a href="/tags/lambda-calculus/">lambda-calculus</a><small>2</small></li>
  
    <li><a href="/tags/logic/">logic</a><small>2</small></li>
  
    <li><a href="/tags/machine-learning/">machine learning</a><small>1</small></li>
  
    <li><a href="/tags/math/">math</a><small>3</small></li>
  
    <li><a href="/tags/ml/">ml</a><small>1</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>2</small></li>
  
    <li><a href="/tags/philosophy/">philosophy</a><small>1</small></li>
  
    <li><a href="/tags/physics/">physics</a><small>2</small></li>
  
    <li><a href="/tags/scala/">scala</a><small>1</small></li>
  
    <li><a href="/tags/shell/">shell</a><small>1</small></li>
  
    <li><a href="/tags/spark/">spark</a><small>1</small></li>
  
    <li><a href="/tags/statistics/">statistics</a><small>3</small></li>
  
    <li><a href="/tags/theology/">theology</a><small>1</small></li>
  
    <li><a href="/tags/vim/">vim</a><small>2</small></li>
  
    <li><a href="/tags/vimperator/">vimperator</a><small>3</small></li>
  
    <li><a href="/tags/zsh/">zsh</a><small>1</small></li>
  
    <li><a href="/tags/ツッコミ/">ツッコミ</a><small>1</small></li>
  
    <li><a href="/tags/中二病/">中二病</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Farseer He
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'githubio';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
</body>
</html>